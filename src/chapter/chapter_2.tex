\chapter{Marco teórico}\label{ch:chapter_2}

\todo[inline]{@Marlene: colocas definiciones en las secciones pero no las referencias de esas definiciones
También debes relacionar cada seccion con tu trabajo Código limpio y arquitectura limpia pueden ir en una sola seccion
Si colocas una figura, debes referenciarla en el texto y explicarla}


\section{Código limpio}

\subsection{Introducción histórica}
El concepto de Código Limpio ha sido fundamental en la programación de software desde los primeros días del desarrollo
de software, pero fue articulado y popularizado con gran efecto por Robert C. Martin en su libro ~\cite{martin2008}.


Este libro se convirtió en una guía esencial para muchos desarrolladores al enfatizar la importancia de escribir código
que no solo funcione, sino que también sea fácil de entender, modificar y mantener.

\subsection{Definición}
Código limpio es aquel que es fácil de entender y fácil de modificar.
Es elegante, eficiente y hace exactamente lo que se espera que haga.
Según Robert C. Martin, el código limpio puede ser leído y mejorado por un
desarrollador que no sea su autor original con un mínimo esfuerzo necesario.

Se caracteriza por su simplicidad, la ausencia de duplicación, la expresión clara de la intención del desarrollador, y
la atención a los detalles en el nivel de código.
Contiene los tests, no contiene errores y, lo más importante, revela claramente su diseño y propósito.


\section{Arquitectura limpia}

\subsection{Introducción histórica}
En el ámbito del desarrollo de software, la estructuración eficiente de la arquitectura de un sistema
es crucial para determinar su eficiencia, escalabilidad y mantenibilidad a largo plazo.

Una de las metodologías que ha cobrado relevancia en este contexto es la Arquitectura Limpia,
un enfoque sistemático para el diseño de software promovido por
Robert C. Martin.
En su libro Clean Architecture: A Craftsman\'
s Guide to Software Structure and Design (Martin, 2017), Martin presenta un marco de trabajo que prioriza la separación
de preocupaciones y la independencia de los diversos componentes del software.

\subsection{Definición}
La arquitectura limpia es un estilo de diseño de software que
organiza el sistema de manera que sea independiente de frameworks, UI, bases de datos y cualquier otra agencia externa.

\todo[inline]{@TODO: Insertar figura}
\todo[inline]{@Marlene: hay que referenciar en el texto la figura. Te falta colocar la referencia}

Representación de las comunicaciones entre capas en una arquitectura limpia.

La esencia de esta arquitectura radica en su diagrama concéntrico de capas, donde cada capa tiene responsabilidades
claramente definidas y depende solo de las capas más internas.
Esto se logra mediante el principio de Inversión de dependencias, lo que significa que los detalles dependen de las
abstracciones y no al contrario.

Aunque existen diferentes implementaciones de una arquitectura limpia, cada una con sus características intrínsecas, en
este proyecto hemos decidido implementar una versión en 3 capas.

\todo[inline]{@Marlene: mencionales. Aparecen 4 a continuación, por qué?}
\subsubsection*{Dominio}
Es el corazón del modelo de negocio de la aplicación y encapsula la
lógica y las reglas del negocio.

Esta capa es fundamentalmente agnóstica respecto a la aplicación de tecnologías externas y se
centra exclusivamente en cómo se comporta el negocio bajo diferentes condiciones y reglas.

Contiene entidades, objetos de valor, y dominios de servicio que representan y operan sobre los
conceptos fundamentales del negocio.

Esta capa debe ser autocontenida y fácilmente testable, aislada de influencias externas como bases de
datos o interfaces de usuario.

\subsubsection*{Aplicación}
La capa de aplicación actúa como un mediador entre la capa de presentación y la capa de dominio, coordinando
las operaciones de alto nivel que involucran múltiples aspectos del dominio.

\subsubsection*{Infraestructura}
La capa de infraestructura proporciona las capacidades tecnológicas necesarias para que las capas de
aplicación y dominio puedan realizar sus funciones sin tener que
preocuparse por los detalles de implementación de la plataforma o los elementos externos.

\subsubsection*{Presentación}
Es responsable de mostrar la información al usuario y de interpretar los
comandos del usuario para que la aplicación pueda entenderlos y actuar en consecuencia.

\todo[inline]{@Marlene: queda mejor si lo redactas como parrafo y no siguiendo ese estilo}


\section{Procesamiento de lenguaje natural}

\subsection{Introducción histórica}
El Procesamiento de Lenguaje Natural o Natural Language Processing (NLP) es un campo que se sitúa en la
intersección de la informática, la inteligencia artificial y la lingüística.

Se dedica al desarrollo de algoritmos y sistemas que permiten a las computadoras entender, interpretar y generar
lenguaje humano de una manera útil y significativa.
La historia del NLP comienza en la década de 1950, marcada por el trabajo pionero de Alan Turing y su famoso
test de Turing, que planteaba la cuestión de si una máquina puede emular el lenguaje humano de manera convincente
(Turing, 1950).

En los años 60 y 70, el enfoque inicial en la traducción automática
, como los esfuerzos del proyecto Georgetown, mostró tanto promesas como limitaciones significativas, lo que llevó a un
reajuste en las expectativas y métodos del campo (Hutchins, 2003).
Con la introducción de la inteligencia artificial (IA) en la década de 1980, surgieron métodos basados primero en reglas
y luego en modelos
estadísticos, culminando con el desarrollo de modelos de aprendizaje automático en la década de 1990 (Manning y Schütze,
1999).

El verdadero cambio paradigmático llegó con el advenimiento de las redes neuronales y el aprendizaje profundo
en la década de 2010.
Este período vio la creación de modelos de lenguaje avanzados, como BERT y GPT, que han revolucionado la capacidad de
las máquinas para procesar el lenguaje con un grado de sutileza y profundidad sin precedentes (Devlin et al., 2019;
Brown et al., 2020).

\subsection{Definición}
El NLP es un campo que combina técnicas de la informática, la inteligencia artificial y la lingüística computacional con
el objetivo de permitir que las máquinas entiendan, interpreten
, manipulen y generen lenguaje humano de manera efectiva y eficiente.

El NLP utiliza algoritmos y modelos matemáticos para abordar diversas tareas relacionadas con el lenguaje, tales como la
traducción automática entre idiomas, la generación de respuestas automáticas
, la extracción de información relevante de textos, el análisis de sentimientos, el reconocimiento de voz, y
la síntesis de habla, entre otros.


\section{Modelos de lenguaje de gran escala (LLMs)}

\subsection{Introducción histórica}
La historia de los LLMs comienza con los primeros modelos estadísticos de lenguaje en
la década de 1980, que utilizaban métodos simples como los modelos de Markov y n-gramas para predecir la probabilidad
de secuencias de palabras (Jelinek, 1997).
Estos métodos, aunque efectivos para algunas tareas básicas, estaban limitados por su incapacidad para capturar contextos
más largos y por su dependencia de grandes corpus de texto para entrenamiento.

En la década de 2000, con el advenimiento de modelos más sofisticados como los modelos ocultos de Markov
y especialmente las redes neuronales, comenzó a vislumbrarse el
potencial de los modelos de lenguaje más complejos.
Sin embargo, no fue hasta la introducción de las redes neuronales recurrentes (RNN) y, más tarde, las redes neuronales
de memoria a largo plazo (LSTM)
que los investigadores pudieron abordar el problema del ''desvanecimiento del gradiente''
y mejorar significativamente la capacidad de los
modelos para aprender dependencias a largo plazo en el texto (Hochreiter Schmidhuber, 1997).

\subsubsection*{La revolución de los transformers}
El verdadero cambio paradigmático llegó en 2017 con el desarrollo de la arquitectura
Transformer por Vaswani et al.
Esta arquitectura introdujo el mecanismo de atención, que permite a los modelos ponderar diferentes partes de la entrada
de texto de manera dinámica, mejorando la capacidad de los modelos para manejar secuencias de texto largas y complejas (
Vaswani et al., 2017).

\todo[inline]{@Marlene: Transformers términos en inglés van en itálicas}

BERT, desarrollado por Google en 2018, y GPT, desarrollado por OpenAI, son ejemplos de
cómo los Transformers han sido adaptados para crear modelos que
no solo entienden el contexto de una palabra en función de su posición en una frase, sino en todo
el texto, permitiendo un entendimiento contextual mucho más rico (Devlin et al., 2019; Radford et al., 2018).

\subsection{Definición}
Los Modelos de Lenguaje de Gran Escala (LLMs) son sistemas
avanzados de inteligencia artificial diseñados para entender, generar y
manipular el lenguaje humano de manera coherente y contextualizada. Estos
modelos son entrenados en vastos conjuntos de datos textuales que abarcan una amplia variedad de temas, lo que les
permite desarrollar un conocimiento extenso sobre el lenguaje y sus usos prácticos.

Funcionan principalmente a través de técnicas de aprendizaje
profundo, especialmente usando arquitecturas como las de Transformers,
que permiten a los modelos captar relaciones complejas y contextos a largo plazo dentro de los textos. Este enfoque les
otorga la capacidad de generar respuestas, completar textos,
traducir entre idiomas, resumir información y responder preguntas con un alto grado de precisión y relevancia.

\subsection{Principales modelos}

\todo[inline]
{@Marlene: puedes hacer un resumen de estos modelos. Creo que no hace falta que coloques los código de ejemplos. Quizás
para un anexo}
\subsubsection*{OpenAI GPT}
ChatGPT es un avanzado modelo de lenguaje desarrollado por OpenAI,
basado en la arquitectura GPT (Generative Pre-trained Transformer).

Es uno de los modelos más reconocidos en la actualidad.
Aunque es un modelo privado, OpenAI ofrece acceso a través de una API de pago que incluye una capa gratuita limitada.

ChatGPT ofrece varios modelos, cada uno con un esquema de precios que varía según
su complejidad.
Una de las principales ventajas de este servicio es que proporciona una solución integral "plug-and-play", es decir, no
requiere instalación ni configuración adicional por parte del usuario.

A continuación un ejemplo de una llamada a esta API.


\subsubsection*{Hugging Face's Transformers}
Hugging Face's Transformers
es una biblioteca de procesamiento de lenguaje natural (NLP) que simplifica el uso de modelos de aprendizaje profundo
basados en la arquitectura Transformer.

Esta biblioteca ofrece una amplia variedad de modelos pre entrenados desarrollados por la comunidad
para fines específicos, además de la posibilidad de entrenar modelos personalizados.


Sin embargo, los modelos disponibles en Hugging Face suelen ser menos potentes que los modelos más
avanzados disponibles a través de plataformas como ChatGPT. ChatGPT ofrece
modelos con capacidades superiores, lo que puede ser una
consideración importante dependiendo de los requisitos específicos del proyecto.

\subsubsection*{Otros}
Además de OpenAI y Hugging Face, existen varias otras plataformas que ofrecen modelos de lenguaje
grande (LLM) para desarrolladores interesados en incorporar capacidades
avanzadas de procesamiento de lenguaje natural en sus aplicaciones. Entre
estas plataformas destacan Google AI, Facebook AI, NVIDIA NeMo y Microsoft AI, entre otras cada una con sus propios
enfoques y modelos distintivos.
